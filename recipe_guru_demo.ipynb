{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zG_vbpsn1cyt",
        "outputId": "9f00dc95-08be-4c99-b29e-c92d876f2f40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing necessary libraries...\n",
            "Importing modules...\n",
            "Ready for the next step!\n"
          ]
        }
      ],
      "source": [
        "print(\"Installing necessary libraries...\")\n",
        "!pip install openai requests -q\n",
        "\n",
        "print(\"Importing modules...\")\n",
        "import openai\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "import sqlite3\n",
        "import time\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "print(\"Ready for the next step!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 2. Setup API Keys and Google Drive ===\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "print(\"Loading API keys from Colab Secrets...\")\n",
        "try:\n",
        "    OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "    SPOONACULAR_API_KEY = userdata.get(\"SPOONACULAR_API_KEY\")\n",
        "    OPENWEATHERMAP_API_KEY = userdata.get(\"OPENWEATHERMAP_API_KEY\")\n",
        "\n",
        "    # Check if keys are loaded\n",
        "    if not all([OPENAI_API_KEY, SPOONACULAR_API_KEY, OPENWEATHERMAP_API_KEY]):\n",
        "        raise ValueError(\"Not all API keys found in Colab Secrets. Please add OPENAI_API_KEY, SPOONACULAR_API_KEY, OPENWEATHERMAP_API_KEY.\")\n",
        "\n",
        "    openai.api_key = OPENAI_API_KEY\n",
        "    print(\"API keys loaded successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading keys or mounting drive: {e}\")\n",
        "    print(\"Please ensure you have added the keys to Colab Secrets and authorized Google Drive access.\")\n",
        "\n",
        "# Define the path to the database file on your Google Drive\n",
        "# You can change the MyDrive/ColabData folder to any other\n",
        "DB_FOLDER = \"/content/drive/MyDrive/ColabData\"\n",
        "if not os.path.exists(DB_FOLDER):\n",
        "    os.makedirs(DB_FOLDER)\n",
        "DB_PATH = os.path.join(DB_FOLDER, \"recipe_assistant_db.sqlite\")\n",
        "print(f\"Database will be stored at: {DB_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "25k5o4RP45_m",
        "outputId": "0765a383-af93-4111-fa22-a787d3db09e0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Loading API keys from Colab Secrets...\n",
            "API keys loaded successfully.\n",
            "Database will be stored at: /content/drive/MyDrive/ColabData/recipe_assistant_db.sqlite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 3. Functions for Working with SQLite Database ===\n",
        "\n",
        "def get_db_connection():\n",
        "    \"\"\"Establishes a connection to the SQLite DB.\"\"\"\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    conn.row_factory = sqlite3.Row # Return rows as dictionaries\n",
        "    return conn\n",
        "\n",
        "def init_db():\n",
        "    \"\"\"Initializes the DB: creates the table if it doesn't exist.\"\"\"\n",
        "    conn = None # Initialize conn\n",
        "    try:\n",
        "        conn = get_db_connection()\n",
        "        cursor = conn.cursor()\n",
        "        # Fixed line breaks in DEFAULT\n",
        "        cursor.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS user_profiles (\n",
        "                user_id TEXT PRIMARY KEY,\n",
        "                preferences_json TEXT DEFAULT \t'{}',\n",
        "                allergies_json TEXT DEFAULT \t'[]'\n",
        "            );\n",
        "        \"\"\")\n",
        "        conn.commit()\n",
        "        print(\"Database initialized successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing DB: {e}\")\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n",
        "\n",
        "def get_user_profile(user_id):\n",
        "    \"\"\"Gets the user profile (preferences and allergies) from the DB.\"\"\"\n",
        "    conn = None\n",
        "    try:\n",
        "        conn = get_db_connection()\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"SELECT preferences_json, allergies_json FROM user_profiles WHERE user_id = ?\", (user_id,))\n",
        "        row = cursor.fetchone()\n",
        "        if row:\n",
        "            preferences = json.loads(row[\"preferences_json\"])\n",
        "            allergies = json.loads(row[\"allergies_json\"])\n",
        "            return json.dumps({\"user_id\": user_id, \"preferences\": preferences, \"allergies\": allergies})\n",
        "        else:\n",
        "            # If the user doesn't exist, create an empty profile\n",
        "            cursor.execute(\"INSERT INTO user_profiles (user_id) VALUES (?)\", (user_id,))\n",
        "            conn.commit()\n",
        "            return json.dumps({\"user_id\": user_id, \"preferences\": {}, \"allergies\": [], \"message\": \"New profile created.\"})\n",
        "    except Exception as e:\n",
        "        print(f\"Error in get_user_profile for {user_id}: {e}\")\n",
        "        return json.dumps({\"error\": f\"Error retrieving user profile for {user_id}\"})\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n",
        "\n",
        "def save_user_preference(user_id, preference_type, value, action=\"add\"):\n",
        "    \"\"\"Adds or removes a user preference (like/dislike).\"\"\"\n",
        "    conn = None\n",
        "    try:\n",
        "        conn = get_db_connection()\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"SELECT preferences_json FROM user_profiles WHERE user_id = ?\", (user_id,))\n",
        "        row = cursor.fetchone()\n",
        "        if not row:\n",
        "            # Important: Close the connection before returning the error\n",
        "            if conn: conn.close()\n",
        "            return json.dumps({\"error\": f\"User {user_id} not found. Profile was not initialized.\"}) # Should be created via get_user_profile first\n",
        "\n",
        "        preferences = json.loads(row[\"preferences_json\"])\n",
        "        if preference_type not in preferences:\n",
        "            preferences[preference_type] = []\n",
        "\n",
        "        item_list = preferences[preference_type]\n",
        "        value_lower = value.lower() # Store in lowercase\n",
        "\n",
        "        if action == \"add\":\n",
        "            if value_lower not in item_list:\n",
        "                item_list.append(value_lower)\n",
        "                # Fixed line breaks in f-string\n",
        "                message = f\"Added \t'{value}' to \t'{preference_type}'.\"\n",
        "            else:\n",
        "                message = f\"\t'{value}' is already in \t'{preference_type}'.\"\n",
        "        elif action == \"remove\":\n",
        "            if value_lower in item_list:\n",
        "                item_list.remove(value_lower)\n",
        "                message = f\"Removed \t'{value}' from \t'{preference_type}'.\"\n",
        "            else:\n",
        "                message = f\"\t'{value}' not found in \t'{preference_type}'.\"\n",
        "        else:\n",
        "             # Important: Close the connection before returning the error\n",
        "             if conn: conn.close()\n",
        "             return json.dumps({\"error\": \"Invalid action, use 'add' or 'remove'.\"})\n",
        "\n",
        "        cursor.execute(\"UPDATE user_profiles SET preferences_json = ? WHERE user_id = ?\", (json.dumps(preferences), user_id))\n",
        "        conn.commit()\n",
        "        return json.dumps({\"user_id\": user_id, \"preferences\": preferences, \"message\": message})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in save_user_preference for {user_id}: {e}\")\n",
        "        return json.dumps({\"error\": f\"Error saving preference for {user_id}\"})\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n",
        "\n",
        "def save_user_allergy(user_id, allergy_value, action=\"add\"):\n",
        "    \"\"\"Adds or removes a user allergy.\"\"\"\n",
        "    conn = None\n",
        "    try:\n",
        "        conn = get_db_connection()\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"SELECT allergies_json FROM user_profiles WHERE user_id = ?\", (user_id,))\n",
        "        row = cursor.fetchone()\n",
        "        if not row:\n",
        "            # Important: Close the connection before returning the error\n",
        "            if conn: conn.close()\n",
        "            return json.dumps({\"error\": f\"User {user_id} not found. Profile was not initialized.\"}) # Should be created via get_user_profile first\n",
        "\n",
        "        allergies = json.loads(row[\"allergies_json\"])\n",
        "        value_lower = allergy_value.lower()\n",
        "\n",
        "        if action == \"add\":\n",
        "            if value_lower not in allergies:\n",
        "                allergies.append(value_lower)\n",
        "                # Fixed line breaks in f-string\n",
        "                message = f\"Allergy added: \t'{allergy_value}'.\"\n",
        "            else:\n",
        "                message = f\"Allergy \t'{allergy_value}' already recorded.\"\n",
        "        elif action == \"remove\":\n",
        "            if value_lower in allergies:\n",
        "                allergies.remove(value_lower)\n",
        "                message = f\"Allergy removed: \t'{allergy_value}'.\"\n",
        "            else:\n",
        "                message = f\"Allergy \t'{allergy_value}' not found.\"\n",
        "        else:\n",
        "            # Important: Close the connection before returning the error\n",
        "            if conn: conn.close()\n",
        "            return json.dumps({\"error\": \"Invalid action, use 'add' or 'remove'.\"})\n",
        "\n",
        "        cursor.execute(\"UPDATE user_profiles SET allergies_json = ? WHERE user_id = ?\", (json.dumps(allergies), user_id))\n",
        "        conn.commit()\n",
        "        return json.dumps({\"user_id\": user_id, \"allergies\": allergies, \"message\": message})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in save_user_allergy for {user_id}: {e}\")\n",
        "        return json.dumps({\"error\": f\"Error saving allergy for {user_id}\"})\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n",
        "\n",
        "# Call DB initialization when the cell runs\n",
        "init_db()\n",
        "print(\"DB functions defined.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-hioVZoF47Ni",
        "outputId": "4e83a61d-da63-4d1e-b61b-484cd44a87e1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database initialized successfully.\n",
            "DB functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 4. Functions for External APIs (Weather, Recipes) ===\n",
        "\n",
        "def get_weather(city):\n",
        "    \"\"\"Gets the current weather for the specified city from OpenWeatherMap.\"\"\"\n",
        "    base_url = \"http://api.openweathermap.org/data/2.5/weather?\"\n",
        "    # Using f-string for cleaner URL construction\n",
        "    complete_url = f\"{base_url}appid={OPENWEATHERMAP_API_KEY}&q={city}&units=metric&lang=en\" # Changed lang to en\n",
        "    try:\n",
        "        response = requests.get(complete_url)\n",
        "        response.raise_for_status() # Check for HTTP errors\n",
        "        data = response.json()\n",
        "        if data.get(\"cod\") != 404 and data.get(\"cod\") != \"404\": # Check for numeric and string 404\n",
        "            main = data.get(\"main\", {})\n",
        "            weather = data.get(\"weather\", [{}])[0]\n",
        "            temp = main.get(\"temp\")\n",
        "            feels_like = main.get(\"feels_like\")\n",
        "            description = weather.get(\"description\")\n",
        "            return json.dumps({\n",
        "                \"city\": city,\n",
        "                \"temperature\": temp,\n",
        "                \"feels_like\": feels_like,\n",
        "                \"description\": description\n",
        "            })\n",
        "        else:\n",
        "            return json.dumps({\"error\": \"City not found\"})\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"OpenWeatherMap API error: {e}\")\n",
        "        return json.dumps({\"error\": f\"Error requesting weather: {e}\"})\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error in get_weather: {e}\")\n",
        "        return json.dumps({\"error\": \"Unexpected error getting weather\"})\n",
        "\n",
        "def find_recipes(weather_description, temperature, user_query=None, cuisine=None, diet=None, intolerances=None, include_ingredients=None, exclude_ingredients=None, max_ready_time=None):\n",
        "    \"\"\"Searches for recipes on Spoonacular considering weather, user query, and other filters.\"\"\"\n",
        "    api_url = \"https://api.spoonacular.com/recipes/complexSearch\"\n",
        "    params = {\n",
        "        \"apiKey\": SPOONACULAR_API_KEY,\n",
        "        \"number\": 3, # Return the top 3 options\n",
        "        \"addRecipeInformation\": False # Don't request full info yet to save quota\n",
        "    }\n",
        "\n",
        "    # --- Smart logic for query formation based on weather and parameters ---\n",
        "    # (IMPORTANT: This part needs refinement and improvement!)\n",
        "    query_parts = [user_query] if user_query else []\n",
        "    if temperature is not None:\n",
        "        if temperature < 10:\n",
        "            query_parts.append(\"warm soup stew bake comfort\")\n",
        "        elif temperature > 25:\n",
        "            query_parts.append(\"cold salad light refreshing grill\")\n",
        "    # Changed \"дождь\" to \"rain\"\n",
        "    if weather_description and \"rain\" in weather_description.lower():\n",
        "        query_parts.append(\"indoor easy\")\n",
        "\n",
        "    effective_query = \" \".join(filter(None, query_parts))\n",
        "    if effective_query:\n",
        "        params[\"query\"] = effective_query\n",
        "\n",
        "    # Add other filters if provided\n",
        "    if cuisine: params[\"cuisine\"] = cuisine\n",
        "    if diet: params[\"diet\"] = diet\n",
        "    # Allergies are passed as a comma-separated string\n",
        "    if intolerances: params[\"intolerances\"] = \",\".join(intolerances)\n",
        "    if include_ingredients: params[\"includeIngredients\"] = \",\".join(include_ingredients)\n",
        "    # Disliked products\n",
        "    if exclude_ingredients: params[\"excludeIngredients\"] = \",\".join(exclude_ingredients)\n",
        "    if max_ready_time: params[\"maxReadyTime\"] = max_ready_time\n",
        "\n",
        "    print(f\"[DEBUG] Request to Spoonacular Complex Search with parameters: {params}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(api_url, params=params)\n",
        "        response.raise_for_status()\n",
        "        recipes = response.json().get(\"results\", [])\n",
        "        if not recipes:\n",
        "            return json.dumps({\"message\": \"No suitable recipes found based on your criteria.\"})\n",
        "        # Return brief info: ID, title, image\n",
        "        return json.dumps([{\"id\": r.get(\"id\"), \"title\": r.get(\"title\"), \"image\": r.get(\"image\", \"\")} for r in recipes])\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Spoonacular API error (Complex Search): {e}\")\n",
        "        return json.dumps({\"error\": f\"Error searching for recipes: {e}\"})\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error in find_recipes: {e}\")\n",
        "        return json.dumps({\"error\": \"Unexpected error searching for recipes\"})\n",
        "\n",
        "def get_recipe_details(recipe_id):\n",
        "    \"\"\"Gets recipe details by its ID from Spoonacular.\"\"\"\n",
        "    api_url = f\"https://api.spoonacular.com/recipes/{recipe_id}/information\"\n",
        "    params = {\"apiKey\": SPOONACULAR_API_KEY, \"includeNutrition\": False}\n",
        "    try:\n",
        "        response = requests.get(api_url, params=params)\n",
        "        response.raise_for_status()\n",
        "        details = response.json()\n",
        "        ingredients = [item.get(\"original\") for item in details.get(\"extendedIngredients\", [])]\n",
        "        instructions = details.get(\"instructions\", \"Instructions not found.\")\n",
        "        # Can add instruction parsing into steps if needed\n",
        "        return json.dumps({\n",
        "            \"title\": details.get(\"title\", \"Untitled\"),\n",
        "            \"ingredients\": ingredients,\n",
        "            \"instructions\": instructions,\n",
        "            \"sourceUrl\": details.get(\"sourceUrl\", \"\")\n",
        "        })\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Spoonacular API error (Get Info): {e}\")\n",
        "        return json.dumps({\"error\": f\"Error getting recipe details {recipe_id}: {e}\"})\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error in get_recipe_details: {e}\")\n",
        "        return json.dumps({\"error\": f\"Unexpected error getting recipe details {recipe_id}\"})\n",
        "\n",
        "print(\"Functions for external APIs defined.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0vKV6EBJ5AQY",
        "outputId": "38ff8bbd-0f73-47ed-e6ba-075a7b3b0f6c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Functions for external APIs defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 5. Setup OpenAI Assistant ===\n",
        "\n",
        "# EXPLICITLY PASS THE KEY WHEN CREATING THE CLIENT\n",
        "try:\n",
        "    client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "    print(\"OpenAI client created successfully.\")\n",
        "except NameError:\n",
        "    print(\"Error: OPENAI_API_KEY variable is not defined. Please run cell 2.\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"Unexpected error creating OpenAI client: {e}\")\n",
        "    raise\n",
        "\n",
        "# Define the tools (functions) the assistant can call\n",
        "tools_list = [\n",
        "    # --- Weather Function ---\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_weather\",\n",
        "            \"description\": \"Get the current weather (temperature and description) for a specified city.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"city\": {\"type\": \"string\", \"description\": \"The city to get the weather for, e.g., Dublin\"}\n",
        "                },\n",
        "                \"required\": [\"city\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    # --- Recipe Functions ---\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"find_recipes\",\n",
        "            \"description\": \"Find recipes suitable for the current weather and user parameters (query, cuisine, diet, allergies, cooking time, include/exclude ingredients).\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"weather_description\": {\"type\": \"string\", \"description\": \"Weather description (e.g., 'clear sky', 'light rain')\"},\n",
        "                    \"temperature\": {\"type\": [\"number\", \"null\"], \"description\": \"Current temperature in Celsius (can be null if weather is unknown)\"},\n",
        "                    \"user_query\": {\"type\": \"string\", \"description\": \"(Optional) Specific user query for a dish (e.g., 'chicken soup', 'something with salmon')\"},\n",
        "                    \"cuisine\": {\"type\": \"string\", \"description\": \"(Optional) Desired cuisine (e.g., 'italian', 'mexican')\"},\n",
        "                    \"diet\": {\"type\": \"string\", \"description\": \"(Optional) Dietary restrictions (e.g., 'vegetarian', 'gluten free')\"},\n",
        "                    \"intolerances\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"(Optional) List of products causing allergies or intolerances (e.g., ['nuts', 'dairy']).\"},\n",
        "                    \"include_ingredients\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"(Optional) List of desired ingredients.\",},\n",
        "                    \"exclude_ingredients\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"(Optional) List of ingredients to exclude (e.g., disliked products).\"},\n",
        "                    \"max_ready_time\": {\"type\": \"integer\", \"description\": \"(Optional) Maximum cooking time in minutes.\",}\n",
        "                },\n",
        "                \"required\": [\"weather_description\", \"temperature\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_recipe_details\",\n",
        "            \"description\": \"Get detailed information about a recipe (ingredients, preparation steps) by its ID.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"recipe_id\": {\"type\": \"integer\", \"description\": \"The ID of the recipe obtained from the find_recipes function\"}\n",
        "                },\n",
        "                \"required\": [\"recipe_id\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    # --- User Profile Functions ---\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_user_profile\",\n",
        "            \"description\": \"Get saved preferences (likes/dislikes) and allergies for the specified user. If the profile doesn't exist, it will be created.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"user_id\": {\"type\": \"string\", \"description\": \"Unique user identifier (e.g., username).\"}\n",
        "                },\n",
        "                \"required\": [\"user_id\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"save_user_preference\",\n",
        "            \"description\": \"Save or remove a user preference (like/dislike) for a specific product or food type.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"user_id\": {\"type\": \"string\", \"description\": \"User identifier.\",},\n",
        "                    \"preference_type\": {\"type\": \"string\", \"enum\": [\"likes\", \"dislikes\"], \"description\": \"Preference type: 'likes' or 'dislikes'.\"},\n",
        "                    \"value\": {\"type\": \"string\", \"description\": \"Product or food type (e.g., 'chicken', 'mushrooms', 'spicy').\"},\n",
        "                    \"action\": {\"type\": \"string\", \"enum\": [\"add\", \"remove\"], \"default\": \"add\", \"description\": \"Action: 'add' or 'remove'. Defaults to 'add'.\"}\n",
        "                },\n",
        "                \"required\": [\"user_id\", \"preference_type\", \"value\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"save_user_allergy\",\n",
        "            \"description\": \"Save or remove information about a user's allergy to a specific product.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"user_id\": {\"type\": \"string\", \"description\": \"User identifier.\",},\n",
        "                    \"allergy_value\": {\"type\": \"string\", \"description\": \"Product the user is allergic to (e.g., 'peanuts', 'milk', 'gluten').\"},\n",
        "                    \"action\": {\"type\": \"string\", \"enum\": [\"add\", \"remove\"], \"default\": \"add\", \"description\": \"Action: 'add' or 'remove'. Defaults to 'add'.\"}\n",
        "                },\n",
        "                \"required\": [\"user_id\", \"allergy_value\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "]\n",
        "\n",
        "# --- Assistant Instructions ---\n",
        "# === Updated Assistant Instructions ===\n",
        "ASSISTANT_INSTRUCTIONS = \"\"\"\n",
        "You are the Weather Recipe Guru, a friendly and creative culinary assistant. Your primary goal is to suggest recipes perfectly suited to the current weather and the user's personal preferences and restrictions. Aim for a natural and efficient conversation.\n",
        "\n",
        "Initial Interaction Flow:\n",
        "1.  **Greeting & Info Gathering:** Start with a friendly greeting. In your *first or second message*, politely ask for the user's name (for their profile, use it as user_id) AND the city they are in (to check the weather).\n",
        "2.  **Profile & Weather:** Once you have the user_id and city:\n",
        "    *   Call `get_user_profile` to load their saved preferences (likes/dislikes) and allergies. Remember these details.\n",
        "    *   Call `get_weather` to get the current temperature and weather description.\n",
        "3.  **Contextual Recipe Query:** Now that you have the profile (if any) and weather, ask the user what kind of meal they're looking for today. *Mention the weather* (e.g., \"It's quite chilly in [City] today...\") and *briefly acknowledge their known preferences/allergies* if available (e.g., \"...and I know you like spicy food and avoid nuts...\") to show you're using the context. Ask for their specific wishes (e.g., 'something quick', 'Italian cuisine', 'use chicken').\n",
        "\n",
        "Recipe Suggestion Workflow:\n",
        "4.  **Recipe Search:**\n",
        "    a.  Analyze the weather (temperature, description) and the user's request.\n",
        "    b.  Call the `find_recipes` function. Always pass `weather_description` and `temperature`. Also include:\n",
        "        -   The user's specific request in `user_query`.\n",
        "        -   Saved allergies in `intolerances`.\n",
        "        -   Saved `dislikes` in `exclude_ingredients`.\n",
        "        -   (Optional) Consider `likes` when forming `user_query` or pass them in `include_ingredients`.\n",
        "        -   Any other user clarifications (cuisine, diet, cooking time).\n",
        "    c.  Suggest 1-3 relevant recipes found (name and image URL if available).\n",
        "5.  **Recipe Details:** If the user selects a recipe, call `get_recipe_details` using its ID. Present the name, ingredients list, and step-by-step instructions clearly.\n",
        "\n",
        "Profile Management:\n",
        "6.  **Saving Preferences/Allergies:** If the user explicitly states a preference or allergy (e.g., 'Remember, I don't like onions', 'I'm allergic to peanuts'), use their user_id to call `save_user_preference` (e.g., type='dislikes', value='onion') or `save_user_allergy` (e.g., allergy_value='peanut') respectively. Confirm briefly that you've saved it.\n",
        "\n",
        "General Guidelines:\n",
        "*   **Be Proactive but Polite:** Gather necessary information efficiently but maintain a friendly tone.\n",
        "*   **Use Context:** Refer back to weather, preferences, and allergies when relevant.\n",
        "*   **Clarify:** If a user request is ambiguous, ask clarifying questions.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "assistant = None # Initialize the assistant variable\n",
        "try:\n",
        "    # Check that the OpenAI client was successfully created earlier\n",
        "    if 'client' not in locals() or client is None:\n",
        "        print(\"Error: OpenAI client was not created. Please restart cell 5.\") # Assuming this code is in cell 5, adjust if needed\n",
        "        raise NameError(\"OpenAI client not initialized\")\n",
        "\n",
        "    assistant = client.beta.assistants.create(\n",
        "        name=\"Weather Recipe Guru v2\", # Translated name\n",
        "        instructions=ASSISTANT_INSTRUCTIONS,\n",
        "        model=\"gpt-4o-mini\", # Using an economical model to start\n",
        "        tools=tools_list,\n",
        "    )\n",
        "    print(f\"Assistant created with ID: {assistant.id}\")\n",
        "except openai.AuthenticationError as auth_err:\n",
        "    print(f\"OpenAI authentication error: {auth_err}. Check your OpenAI API key in Colab Secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating assistant: {e}\")\n",
        "    # Attempt to find an existing assistant by name (if creation failed)\n",
        "    try:\n",
        "        print(\"Attempting to find an existing assistant...\")\n",
        "        my_assistants = client.beta.assistants.list(order=\"desc\", limit=\"20\")\n",
        "        found = False # Flag indicating assistant found\n",
        "        for existing_assistant in my_assistants.data:\n",
        "            if existing_assistant.name == \"Weather Recipe Guru v2\": # Use translated name\n",
        "                assistant = existing_assistant\n",
        "                print(f\"Found existing assistant with ID: {assistant.id}\")\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "             print(\"Failed to create or find an assistant named 'Weather Recipe Guru v2'.\")\n",
        "    except Exception as list_e:\n",
        "        print(f\"Error searching for existing assistants: {list_e}\")\n",
        "\n",
        "# Add a check that assistant is not None before using\n",
        "if assistant:\n",
        "    print(f\"Using assistant: {assistant.name} (ID: {assistant.id})\")\n",
        "else:\n",
        "    print(\"\\n!!! WARNING: Assistant was not created or found. Subsequent code will not work correctly. Check errors above and your OpenAI API key. !!!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9intV0qj5K_s",
        "outputId": "ed5c84ec-9e02-4cf8-c56b-101857a572ee"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI client created successfully.\n",
            "Assistant created with ID: asst_36JdysTVWsHUJHfz3DzIXloq\n",
            "Using assistant: Weather Recipe Guru v2 (ID: asst_36JdysTVWsHUJHfz3DzIXloq)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 6. Assistant Interaction Loop ===\n",
        "\n",
        "import json\n",
        "import time\n",
        "\n",
        "# --- Helper Functions for API Calls ---\n",
        "# Note: These API call functions (get_weather, find_recipes, get_recipe_details)\n",
        "# are defined here for clarity within the interaction loop cell.\n",
        "# The user profile functions (get_user_profile, save_user_preference, save_user_allergy)\n",
        "# are NOT redefined here; this cell will use the versions defined in cell 3 (SQLite).\n",
        "\n",
        "def get_weather(city):\n",
        "    \"\"\"Gets weather from OpenWeatherMap API.\"\"\"\n",
        "    print(f\"\\n[Function] Requesting weather for city: {city}\")\n",
        "    base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n",
        "    params = {\n",
        "        'q': city,\n",
        "        'appid': OPENWEATHERMAP_API_KEY, # Use key from cell 2\n",
        "        'units': 'metric', # Celsius degrees\n",
        "        'lang': 'en' # English language for description\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params)\n",
        "        response.raise_for_status() # Check for HTTP errors\n",
        "        data = response.json()\n",
        "\n",
        "        # Extract necessary data\n",
        "        temperature = data.get('main', {}).get('temp')\n",
        "        description = data.get('weather', [{}])[0].get('description')\n",
        "\n",
        "        if temperature is not None and description:\n",
        "            result = {\"temperature\": temperature, \"description\": description}\n",
        "            print(f\"[Function] Weather received: {result}\")\n",
        "            return json.dumps(result) # Return JSON string\n",
        "        else:\n",
        "            print(\"[Function] Error: Failed to extract temperature or description from API response.\")\n",
        "            return json.dumps({\"error\": \"Failed to get complete weather data.\"})\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"[Function] Weather API error: {e}\")\n",
        "        return json.dumps({\"error\": f\"Error requesting weather API: {e}\"})\n",
        "    except Exception as e:\n",
        "        print(f\"[Function] Unexpected error in get_weather: {e}\")\n",
        "        return json.dumps({\"error\": \"Unexpected error getting weather.\"})\n",
        "\n",
        "def find_recipes(weather_description, temperature, user_query=None, cuisine=None, diet=None, intolerances=None, include_ingredients=None, exclude_ingredients=None, max_ready_time=None):\n",
        "    \"\"\"Searches for recipes via Spoonacular API.\"\"\"\n",
        "    print(f\"\\n[Function] Searching recipes. Weather: {weather_description}, {temperature}°C. Query: {user_query}\")\n",
        "    base_url = \"https://api.spoonacular.com/recipes/complexSearch\"\n",
        "    params = {\n",
        "        'apiKey': SPOONACULAR_API_KEY, # Use key from cell 2\n",
        "        'query': user_query if user_query else weather_description, # Use weather description as default query\n",
        "        'cuisine': cuisine,\n",
        "        'diet': diet,\n",
        "        'intolerances': ','.join(intolerances)   if intolerances else None, # API expects comma-separated string\n",
        "        'includeIngredients': ','.join(include_ingredients) if include_ingredients else None,\n",
        "        'excludeIngredients': ','.join(exclude_ingredients) if exclude_ingredients else None,\n",
        "        'maxReadyTime': max_ready_time,\n",
        "        'number': 3, # Request 3 recipes for choice\n",
        "        'addRecipeInformation': True # Include basic recipe info in response\n",
        "    }\n",
        "\n",
        "    # Add some logic based on weather for automatic filtering\n",
        "    if temperature is not None:\n",
        "        if temperature > 25: # Hot\n",
        "            # Search for light dishes, salads, cold soups\n",
        "            params['query'] = user_query if user_query else 'salad OR cold soup OR light meal'\n",
        "            params['maxCalories'] = 600\n",
        "        elif temperature < 5: # Cold\n",
        "            # Search for warming dishes, soups, stews\n",
        "            params['query'] = user_query if user_query else 'soup OR stew OR hot meal OR warming'\n",
        "            params['minCalories'] = 400\n",
        "\n",
        "    # Remove None parameters for a clean request\n",
        "    params = {k: v for k, v in params.items() if v is not None}\n",
        "\n",
        "    print(f\"[Function] Parameters for Spoonacular request: {params}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        # Format result for the assistant\n",
        "        recipes = []\n",
        "        for recipe in data.get('results', []):\n",
        "            recipes.append({\n",
        "                \"id\": recipe.get('id'),\n",
        "                \"title\": recipe.get('title'),\n",
        "                \"image\": recipe.get('image'), # Image URL\n",
        "                \"readyInMinutes\": recipe.get('readyInMinutes')\n",
        "            })\n",
        "\n",
        "        print(f\"[Function] Recipes found: {len(recipes)}\")\n",
        "        if not recipes:\n",
        "             print(\"[Function] No recipes found for the given criteria.\")\n",
        "             return json.dumps({\"message\": \"Unfortunately, nothing was found for your request. Try changing the criteria.\"})\n",
        "        return json.dumps(recipes) # Return JSON string with recipe list\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"[Function] Recipe API error: {e}\")\n",
        "        # Attempt to extract error message from Spoonacular\n",
        "        error_message = f\"Error requesting recipe API: {e}\"\n",
        "        try:\n",
        "            error_data = response.json()\n",
        "            if 'message' in error_data:\n",
        "                error_message += f\" (API Message: {error_data['message']})\"\n",
        "        except: # Ignore errors parsing JSON error response\n",
        "            pass\n",
        "        return json.dumps({\"error\": error_message})\n",
        "    except Exception as e:\n",
        "        print(f\"[Function] Unexpected error in find_recipes: {e}\")\n",
        "        return json.dumps({\"error\": \"Unexpected error searching for recipes.\"})\n",
        "\n",
        "def get_recipe_details(recipe_id):\n",
        "    \"\"\"Gets recipe details (ingredients, steps) by ID.\"\"\"\n",
        "    print(f\"\\n[Function] Requesting details for recipe ID: {recipe_id}\")\n",
        "    base_url = f\"https://api.spoonacular.com/recipes/{recipe_id}/information\"\n",
        "    params = {\n",
        "        'apiKey': SPOONACULAR_API_KEY,\n",
        "        'includeNutrition': False # Don't request nutrition info to save quota\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        # Extract and format necessary data\n",
        "        details = {\n",
        "            \"title\": data.get('title'),\n",
        "            \"readyInMinutes\": data.get('readyInMinutes'),\n",
        "            \"servings\": data.get('servings'),\n",
        "            \"ingredients\": [item.get('original') for item in data.get('extendedIngredients', [])],\n",
        "            \"instructions\": data.get('instructions'), # Can be HTML or text\n",
        "            \"sourceUrl\": data.get('sourceUrl')\n",
        "        }\n",
        "\n",
        "        # Attempt to extract instruction steps if structured\n",
        "        analyzed_instructions = data.get('analyzedInstructions', [])\n",
        "        if analyzed_instructions and len(analyzed_instructions) > 0:\n",
        "            steps = []\n",
        "            for step_data in analyzed_instructions[0].get('steps', []):\n",
        "                steps.append(f\"{step_data.get('number')}. {step_data.get('step')}\")\n",
        "            if steps:\n",
        "                details[\"instructions_steps\"] = steps # Add separate field with step list\n",
        "\n",
        "        print(f\"[Function] Details for recipe '{details['title']}' received.\")\n",
        "        return json.dumps(details) # Return JSON string\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"[Function] Recipe details API error: {e}\")\n",
        "        error_message = f\"Error requesting recipe details API: {e}\"\n",
        "        try:\n",
        "            error_data = response.json()\n",
        "            if 'message' in error_data:\n",
        "                error_message += f\" (API Message: {error_data['message']})\"\n",
        "        except:\n",
        "            pass\n",
        "        return json.dumps({\"error\": error_message})\n",
        "    except Exception as e:\n",
        "        print(f\"[Function] Unexpected error in get_recipe_details: {e}\")\n",
        "        return json.dumps({\"error\": \"Unexpected error getting recipe details.\"})\n",
        "\n",
        "# --- Main Interaction Loop ---\n",
        "\n",
        "def run_assistant_interaction():\n",
        "    # Check that the assistant was created in the previous cell\n",
        "    if 'assistant' not in globals() or assistant is None:\n",
        "        print(\"Error: Assistant not created or found. Please run cell 5.\")\n",
        "        return\n",
        "\n",
        "    # Check for API keys\n",
        "    if 'OPENWEATHERMAP_API_KEY' not in globals() or not OPENWEATHERMAP_API_KEY:\n",
        "        print(\"Error: OpenWeatherMap API key not found. Please run cell 2.\")\n",
        "        return\n",
        "    if 'SPOONACULAR_API_KEY' not in globals() or not SPOONACULAR_API_KEY:\n",
        "        print(\"Error: Spoonacular API key not found. Please run cell 2.\")\n",
        "        return\n",
        "\n",
        "    # Create a new dialogue (Thread) for this session\n",
        "    try:\n",
        "        thread = client.beta.threads.create()\n",
        "        print(f\"\\nNew thread created with ID: {thread.id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating thread: {e}\")\n",
        "        return\n",
        "\n",
        "    # Dictionary to map function names to their actual implementations\n",
        "    # These names will resolve to the functions defined in cell 3 (SQLite) and cell 6 (APIs)\n",
        "    available_functions = {\n",
        "        \"get_weather\": get_weather,\n",
        "        \"find_recipes\": find_recipes,\n",
        "        \"get_recipe_details\": get_recipe_details,\n",
        "        \"get_user_profile\": get_user_profile, # Will use the one from cell 3\n",
        "        \"save_user_preference\": save_user_preference, # Will use the one from cell 3\n",
        "        \"save_user_allergy\": save_user_allergy, # Will use the one from cell 3\n",
        "    }\n",
        "\n",
        "    print(\"\\n--- Weather Recipe Guru Ready --- \")\n",
        "    print(\"Enter your message or 'exit' to finish.\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"\\nYou: \")\n",
        "            if user_input.lower() == 'exit':\n",
        "                print(\"\\nGoodbye!\")\n",
        "                break\n",
        "\n",
        "            # Send user message to the thread\n",
        "            message = client.beta.threads.messages.create(\n",
        "                thread_id=thread.id,\n",
        "                role=\"user\",\n",
        "                content=user_input\n",
        "            )\n",
        "\n",
        "            # Run the assistant to process the message\n",
        "            run = client.beta.threads.runs.create(\n",
        "                thread_id=thread.id,\n",
        "                assistant_id=assistant.id,\n",
        "                # Can pass additional instructions for this specific run if needed\n",
        "                # instructions=\"Please be especially polite.\"\n",
        "            )\n",
        "            print(f\"\\nRunning assistant... (Run ID: {run.id})\")\n",
        "\n",
        "            # ===>>> LOOP TO WAIT FOR RUN COMPLETION OR ACTION <<<===\n",
        "            while True: # Keep checking the run status\n",
        "                run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
        "                print(f\"Status: {run.status}\", end='\\r')\n",
        "\n",
        "                if run.status == 'completed':\n",
        "                    print(f\"Status: {run.status}      \") # Clear status line\n",
        "                    # Get messages added by the assistant\n",
        "                    messages = client.beta.threads.messages.list(thread_id=thread.id, order=\"asc\", after=message.id) # Get only new messages\n",
        "                    for msg in messages.data:\n",
        "                        if msg.role == \"assistant\":\n",
        "                            # Extract the response text\n",
        "                            for content_block in msg.content:\n",
        "                                if content_block.type == 'text':\n",
        "                                    print(f\"\\nAssistant: {content_block.text.value}\")\n",
        "                    break # Exit the inner while loop, wait for next user input\n",
        "\n",
        "                elif run.status == 'requires_action':\n",
        "                    print(f\"Status: {run.status}      \") # Clear status line\n",
        "                    print(\"\\nAssistant requires function calls...\")\n",
        "                    tool_outputs = [] # List to store function call results\n",
        "\n",
        "                    # Assistant might request multiple function calls at once\n",
        "                    for tool_call in run.required_action.submit_tool_outputs.tool_calls:\n",
        "                        function_name = tool_call.function.name\n",
        "                        function_args = json.loads(tool_call.function.arguments) # Arguments come as JSON string\n",
        "\n",
        "                        print(f\"  - Calling function: {function_name}\")\n",
        "                        print(f\"    Arguments: {function_args}\")\n",
        "\n",
        "                        # Find the function in our dictionary\n",
        "                        if function_name in available_functions:\n",
        "                            function_to_call = available_functions[function_name]\n",
        "                            try:\n",
        "                                # Call the actual function with unpacked arguments\n",
        "                                function_response = function_to_call(**function_args)\n",
        "                                # Print the beginning of the result\n",
        "                                print(f\"    Result: {str(function_response)[:200]}...\" if len(str(function_response)) > 200 else f\"    Result: {function_response}\")\n",
        "\n",
        "                                # Add result to the list for sending back to the assistant\n",
        "                                tool_outputs.append({\n",
        "                                    \"tool_call_id\": tool_call.id,\n",
        "                                    \"output\": str(function_response), # Result must be a string\n",
        "                                })\n",
        "                            except Exception as e:\n",
        "                                print(f\"    Error executing function {function_name}: {e}\")\n",
        "                                # Inform the assistant about the error\n",
        "                                tool_outputs.append({\n",
        "                                    \"tool_call_id\": tool_call.id,\n",
        "                                    \"output\": json.dumps({\"error\": f\"Error executing function {function_name}: {e}\"})\n",
        "                                })\n",
        "                        else:\n",
        "                            print(f\"    Error: Function '{function_name}' not found!\")\n",
        "                            tool_outputs.append({\n",
        "                                \"tool_call_id\": tool_call.id,\n",
        "                                \"output\": json.dumps({\"error\": f\"Function '{function_name}' not defined.\"})\n",
        "                            })\n",
        "\n",
        "                    # Submit the results back to the assistant\n",
        "                    if tool_outputs:\n",
        "                        try:\n",
        "                            run = client.beta.threads.runs.submit_tool_outputs(\n",
        "                                thread_id=thread.id,\n",
        "                                run_id=run.id,\n",
        "                                tool_outputs=tool_outputs\n",
        "                            )\n",
        "                            print(\"\\nSubmitting function results...\")\n",
        "                            # Continue the inner while loop to wait for the next status\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error submitting tool outputs: {e}\")\n",
        "                            break # Exit the inner while loop on submission error\n",
        "                    else:\n",
        "                        print(\"No tool outputs generated.\")\n",
        "                        break # Exit the inner while loop if no tools to submit\n",
        "\n",
        "                elif run.status in ['queued', 'in_progress', 'cancelling']:\n",
        "                    time.sleep(1) # Wait before checking status again\n",
        "                    continue # Continue the inner while loop\n",
        "\n",
        "                elif run.status == 'failed':\n",
        "                    print(f\"Status: {run.status}      \") # Clear status line\n",
        "                    print(f\"\\nRun failed: {run.last_error.message if run.last_error else 'Unknown error'}\")\n",
        "                    break # Exit the inner while loop\n",
        "                elif run.status == 'cancelled':\n",
        "                    print(f\"Status: {run.status}      \") # Clear status line\n",
        "                    print(\"\\nRun cancelled.\")\n",
        "                    break # Exit the inner while loop\n",
        "                elif run.status == 'expired':\n",
        "                    print(f\"Status: {run.status}      \") # Clear status line\n",
        "                    print(\"\\nRun expired.\")\n",
        "                    break # Exit the inner while loop\n",
        "                else:\n",
        "                    print(f\"Status: {run.status}      \") # Clear status line\n",
        "                    print(f\"\\nUnexpected run status: {run.status}\")\n",
        "                    break # Exit the inner while loop\n",
        "            # ===>>> END OF INNER WHILE LOOP <<<===\n",
        "\n",
        "            # Check if the inner loop was broken due to failure/cancellation/etc.\n",
        "            if run.status not in ['completed', 'requires_action']: # If run didn't complete normally or need more action\n",
        "                 break # Exit the main outer while loop\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nInteraction interrupted by user.\")\n",
        "            # Optionally cancel the run if it's in progress\n",
        "            try:\n",
        "                if 'run' in locals() and run.status in ['queued', 'in_progress']:\n",
        "                    client.beta.threads.runs.cancel(thread_id=thread.id, run_id=run.id)\n",
        "                    print(\"Attempted to cancel the current run.\")\n",
        "            except Exception as cancel_e:\n",
        "                print(f\"Error cancelling run: {cancel_e}\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"\\nAn unexpected error occurred: {e}\")\n",
        "            # Log the error or handle it as needed\n",
        "            break # Exit loop on unexpected error\n",
        "\n",
        "\n",
        "print(\"\\nInteraction loop function defined. Uncomment 'run_assistant_interaction()' at the end to start.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EnH4P5PO5Q7N",
        "outputId": "60f2d802-013b-496d-cd20-a2aef95e97c3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Interaction loop function defined. Uncomment 'run_assistant_interaction()' at the end to start.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Start the interaction ---\n",
        "# Call this function to start chatting with the assistant\n",
        "run_assistant_interaction()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "P-JdQRY64dkt",
        "outputId": "9b5d1c4b-25c9-4503-e517-d3dbc03bce49"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "New thread created with ID: thread_O74Du6Jkwrz9wDsnN15NfhSE\n",
            "\n",
            "--- Weather Recipe Guru Ready --- \n",
            "Enter your message or 'exit' to finish.\n",
            "\n",
            "You: Suggest a healthy dinner suitable for the current weather in Dublin.\n",
            "\n",
            "Running assistant... (Run ID: run_oE91dTEaYx6OyhWVolx5WlZ1)\n",
            "Status: completed      \n",
            "\n",
            "Assistant: Hello! I'd be happy to help you with that. First, could you please share your name and confirm that you're currently in Dublin?\n",
            "\n",
            "You: Arsenii, yeah in Dublin rn \n",
            "\n",
            "Running assistant... (Run ID: run_z48GHQvWwIwvkMBJInBNPm5X)\n",
            "Status: requires_action      \n",
            "\n",
            "Assistant requires function calls...\n",
            "  - Calling function: get_user_profile\n",
            "    Arguments: {'user_id': 'Arsenii'}\n",
            "    Result: {\"user_id\": \"Arsenii\", \"preferences\": {\"likes\": [\"italian\"], \"dislikes\": [\"spicy\"]}, \"allergies\": [\"shellfish\"]}\n",
            "  - Calling function: get_weather\n",
            "    Arguments: {'city': 'Dublin'}\n",
            "\n",
            "[Function] Requesting weather for city: Dublin\n",
            "[Function] Weather received: {'temperature': 18.57, 'description': 'clear sky'}\n",
            "    Result: {\"temperature\": 18.57, \"description\": \"clear sky\"}\n",
            "\n",
            "Submitting function results...\n",
            "Status: completed      \n",
            "\n",
            "Assistant: Great to meet you, Arsenii! It's a lovely 18.5°C with a clear sky in Dublin today. I see that you enjoy Italian cuisine and prefer to avoid spicy dishes. Plus, I’ve noted that you are allergic to shellfish.\n",
            "\n",
            "What type of meal are you in the mood for today? Perhaps something fresh and light, or maybe a comforting Italian dish? Let me know your specific wishes!\n",
            "\n",
            "Interaction interrupted by user.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === File Search (Vector Stores via REST API) ===\n",
        "\n",
        "import openai\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import requests # Will be used for Vector Store operations\n",
        "\n",
        "# --- Check OpenAI Version ---\n",
        "try:\n",
        "    print(f\"Using OpenAI version: {openai.__version__}\")\n",
        "except NameError:\n",
        "    print(\"Error: openai module not imported?\")\n",
        "    import openai\n",
        "    print(f\"Using OpenAI version (after import): {openai.__version__}\")\n",
        "\n",
        "# --- Create RAG file with Cooking Tips ---\n",
        "rag_file_content_tips = \"\"\"## Basic Cooking Techniques & Tips\n",
        "\n",
        "### How to Dice an Onion\n",
        "1. Cut the onion in half lengthwise (pole to pole).\n",
        "2. Peel off the skin.\n",
        "3. Place one half flat-side down. Make several horizontal cuts towards the root, but not all the way through it.\n",
        "4. Make several vertical cuts lengthwise, again not cutting through the root.\n",
        "5. Finally, cut crosswise to produce diced pieces. The root end holds it together.\n",
        "\n",
        "### How to Boil an Egg\n",
        "- **Soft-boiled:** Place eggs in boiling water. Boil for 4-6 minutes. Plunge into ice water.\n",
        "- **Hard-boiled:** Place eggs in cold water, bring to a boil. Once boiling, turn off heat, cover, and let sit for 10-12 minutes. Plunge into ice water.\n",
        "\n",
        "### Common Ingredient Substitutions\n",
        "*   **Buttermilk:** For 1 cup, use 1 cup milk + 1 tablespoon lemon juice or white vinegar. Let stand for 5 minutes.\n",
        "*   **Sour Cream:** Plain yogurt or Greek yogurt can often be substituted 1:1.\n",
        "*   **Vegetable Oil (in baking):** Melted butter (1:1), applesauce (1:1, may affect moisture), or mashed banana (1:1).\n",
        "*   **Self-Rising Flour:** For 1 cup, use 1 cup all-purpose flour + 1.5 teaspoons baking powder + 0.25 teaspoon salt.\n",
        "\n",
        "### Food Safety Reminders\n",
        "*   Always wash hands before and after handling raw meat.\n",
        "*   Use separate cutting boards for raw meat and produce.\n",
        "*   Cook chicken to an internal temperature of 74°C (165°F).\n",
        "*   Refrigerate leftovers promptly.\n",
        "\"\"\"\n",
        "\n",
        "# Define the new file path\n",
        "new_rag_file_path = \"/home/ubuntu/cooking_tips_rag.txt\"\n",
        "\n",
        "# Make sure the directory exists (important in some environments)\n",
        "os.makedirs(os.path.dirname(new_rag_file_path), exist_ok=True)\n",
        "\n",
        "# Write the content to the file\n",
        "with open(new_rag_file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(rag_file_content_tips)\n",
        "\n",
        "print(f\"File for RAG '{new_rag_file_path}' created/overwritten.\")\n",
        "\n",
        "# --- Configuration ---\n",
        "ASSISTANT_ID = None\n",
        "if 'assistant' in globals() and assistant is not None:\n",
        "    ASSISTANT_ID = assistant.id\n",
        "    print(f\"Using Assistant ID from cell 5: {ASSISTANT_ID}\")\n",
        "else:\n",
        "    print(\"WARNING: Could not find Assistant ID from cell 5. Set ASSISTANT_ID manually!\")\n",
        "    # ASSISTANT_ID = \"asst_...\" # Insert ID here if needed\n",
        "\n",
        "FILE_PATH = \"/home/ubuntu/cooking_tips_rag.txt\"\n",
        "VECTOR_STORE_NAME = \"Seasonal Produce Ireland Store REST\"\n",
        "\n",
        "# --- REST API Helper Functions --- #\n",
        "\n",
        "def rest_api_request(method, url, api_key, payload=None):\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"OpenAI-Beta\": \"assistants=v2\" # Key header for v2\n",
        "    }\n",
        "    try:\n",
        "        if method.upper() == 'POST':\n",
        "            response = requests.post(url, headers=headers, json=payload)\n",
        "        elif method.upper() == 'GET':\n",
        "            response = requests.get(url, headers=headers)\n",
        "        elif method.upper() == 'DELETE':\n",
        "            response = requests.delete(url, headers=headers)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported HTTP method: {method}\")\n",
        "\n",
        "        # Attempt to decode JSON if there is a response body\n",
        "        response_data = None\n",
        "        if response.text:\n",
        "            try:\n",
        "                response_data = response.json()\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"   [REST] Warning: Non-JSON response received (Status: {response.status_code}): {response.text[:500]}...\")\n",
        "                response_data = {\"error_text\": response.text}\n",
        "\n",
        "        return response.status_code, response_data\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"   [REST] Network Error: {e}\")\n",
        "        return 500, {\"error\": str(e)}\n",
        "    except Exception as e:\n",
        "        print(f\"   [REST] Unexpected Error: {e}\")\n",
        "        return 500, {\"error\": str(e)}\n",
        "\n",
        "# --- Check API Key and Assistant ID Prerequisites ---\n",
        "def check_prerequisites_rest():\n",
        "    if 'OPENAI_API_KEY' not in globals() or not OPENAI_API_KEY:\n",
        "        print(\"Error: OPENAI_API_KEY variable not found. Please run cell 2.\")\n",
        "        return False\n",
        "    if not ASSISTANT_ID:\n",
        "         print(\"Error: ASSISTANT_ID is not defined. Check the beginning of this cell or the output of cell 5.\")\n",
        "         return False\n",
        "    if not os.path.exists(FILE_PATH):\n",
        "        print(f\"Error: RAG file not found at path: {FILE_PATH}\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# --- Main Logic for Updating with Vector Store via REST API ---\n",
        "def setup_file_search_via_rest():\n",
        "    if not check_prerequisites_rest():\n",
        "        return\n",
        "\n",
        "    local_client = None\n",
        "    uploaded_file_id = None\n",
        "    vector_store_id = None\n",
        "    api_key = OPENAI_API_KEY\n",
        "\n",
        "    try:\n",
        "        # Use SDK to upload the file, as that worked\n",
        "        print(\"Creating OpenAI client (for file upload)...\")\n",
        "        local_client = openai.OpenAI(api_key=api_key)\n",
        "        print(\"Client created.\")\n",
        "\n",
        "        # --- Step 1: Upload File (via SDK) ---\n",
        "        print(f\"1. Uploading file {FILE_PATH} via SDK...\")\n",
        "        with open(FILE_PATH, \"rb\") as file_to_upload:\n",
        "            file_object = local_client.files.create(file=file_to_upload, purpose='assistants')\n",
        "        uploaded_file_id = file_object.id\n",
        "        print(f\"   File uploaded successfully, File ID: {uploaded_file_id}\")\n",
        "\n",
        "        # --- Step 2: Create Vector Store (via REST API) --- #\n",
        "        print(f\"2. Creating Vector Store '{VECTOR_STORE_NAME}' via REST API...\")\n",
        "        vs_create_url = \"https://api.openai.com/v1/vector_stores\"\n",
        "        vs_payload = {\"name\": VECTOR_STORE_NAME}\n",
        "        status_code, vs_data = rest_api_request('POST', vs_create_url, api_key, payload=vs_payload)\n",
        "\n",
        "        if status_code == 200 and vs_data and 'id' in vs_data:\n",
        "            vector_store_id = vs_data['id']\n",
        "            print(f\"   Vector Store created, ID: {vector_store_id}\")\n",
        "        else:\n",
        "            print(f\"   Error creating Vector Store (Status: {status_code}). Response:\")\n",
        "            print(json.dumps(vs_data, indent=2))\n",
        "            raise Exception(\"Failed to create Vector Store via REST API.\")\n",
        "\n",
        "        # --- Step 3: Add File to Vector Store (via REST API) --- #\n",
        "        print(f\"3. Adding file {uploaded_file_id} to Vector Store {vector_store_id} via REST API...\")\n",
        "        vs_add_file_url = f\"https://api.openai.com/v1/vector_stores/{vector_store_id}/files\"\n",
        "        vs_add_payload = {\"file_id\": uploaded_file_id}\n",
        "        status_code, vs_file_data = rest_api_request('POST', vs_add_file_url, api_key, payload=vs_add_payload)\n",
        "\n",
        "        if status_code == 200 and vs_file_data and 'id' in vs_file_data:\n",
        "            vs_file_id = vs_file_data['id'] # This is the ID of the file *within* the VS\n",
        "            print(f\"   File {vs_file_id} added to Vector Store. Status: {vs_file_data.get('status', 'N/A')}\")\n",
        "        else:\n",
        "            print(f\"   Error adding file to Vector Store (Status: {status_code}). Response:\")\n",
        "            print(json.dumps(vs_file_data, indent=2))\n",
        "            raise Exception(\"Failed to add file to Vector Store via REST API.\")\n",
        "\n",
        "        # --- Step 4: Wait for File Processing in Vector Store (via REST API) --- #\n",
        "        print(f\"4. Waiting for file {vs_file_id} processing in Vector Store (status 'completed')...\")\n",
        "        vs_file_status_url = f\"https://api.openai.com/v1/vector_stores/{vector_store_id}/files/{vs_file_id}\"\n",
        "        polling_interval = 5\n",
        "        max_wait_time = 180\n",
        "        start_time = time.time()\n",
        "        while True:\n",
        "            status_code, file_status_data = rest_api_request('GET', vs_file_status_url, api_key)\n",
        "            current_status = file_status_data.get('status', 'unknown') if file_status_data else 'error'\n",
        "            print(f\"   Current status of file in VS: {current_status}\", end='\\r')\n",
        "\n",
        "            if current_status == 'completed':\n",
        "                print(\"\\n   File processed successfully in Vector Store.\")\n",
        "                break\n",
        "            elif current_status in ['failed', 'cancelled']:\n",
        "                print(f\"\\n   Error processing file in Vector Store. Status: {current_status}\")\n",
        "                last_error = file_status_data.get('last_error', None)\n",
        "                if last_error:\n",
        "                    print(f\"      Error: {last_error.get('code', 'N/A')} - {last_error.get('message', 'N/A')}\")\n",
        "                raise Exception(f\"Error processing file in Vector Store: {current_status}\")\n",
        "            elif status_code != 200:\n",
        "                 print(f\"\\n   Error checking file status (Status: {status_code}). Response:\")\n",
        "                 print(json.dumps(file_status_data, indent=2))\n",
        "                 raise Exception(\"Failed to check file status in Vector Store.\")\n",
        "\n",
        "            if time.time() - start_time > max_wait_time:\n",
        "                 print(\"\\n   Timeout exceeded waiting for file processing in Vector Store.\")\n",
        "                 raise TimeoutError(\"File was not processed in Vector Store within the allowed time.\")\n",
        "\n",
        "            time.sleep(polling_interval)\n",
        "\n",
        "        # --- Step 5: Update Assistant (via SDK if supports tool_resources, else REST) --- #\n",
        "        print(f\"5. Updating assistant {ASSISTANT_ID} to use File Search and Vector Store {vector_store_id}...\")\n",
        "\n",
        "        tool_resources_payload = {\n",
        "            \"file_search\": {\n",
        "                \"vector_store_ids\": [vector_store_id]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Attempt update via SDK\n",
        "        try:\n",
        "            print(\"   Attempting update via SDK...\")\n",
        "            assistant_details = local_client.beta.assistants.retrieve(ASSISTANT_ID)\n",
        "            current_tools = [tool.to_dict() for tool in assistant_details.tools] if assistant_details.tools else []\n",
        "            updated_tools = list(current_tools)\n",
        "            if not any(tool.get('type') == 'file_search' for tool in updated_tools):\n",
        "                updated_tools.append({\"type\": \"file_search\"})\n",
        "                print(\"      'file_search' tool added.\")\n",
        "\n",
        "            updated_assistant = local_client.beta.assistants.update(\n",
        "                assistant_id=ASSISTANT_ID,\n",
        "                tools=updated_tools,\n",
        "                tool_resources=tool_resources_payload\n",
        "            )\n",
        "            print(f\"   Assistant '{updated_assistant.name}' updated successfully via SDK!\")\n",
        "            update_success = True\n",
        "        except (TypeError, AttributeError) as sdk_err:\n",
        "            print(f\"   SDK {openai.__version__} does not support 'tool_resources'. Error: {sdk_err}\")\n",
        "            print(\"   Falling back to update via REST API...\")\n",
        "            update_success = False\n",
        "        except Exception as sdk_update_err:\n",
        "             print(f\"   Error updating assistant via SDK: {sdk_update_err}\")\n",
        "             update_success = False\n",
        "\n",
        "        # If SDK failed, use REST\n",
        "        if not update_success:\n",
        "            assistant_update_url = f\"https://api.openai.com/v1/assistants/{ASSISTANT_ID}\"\n",
        "            # Get current assistant data via REST for merging\n",
        "            print(\"      Getting current assistant data via REST...\")\n",
        "            get_status, current_assistant_data = rest_api_request('GET', assistant_update_url, api_key)\n",
        "            if get_status != 200 or not current_assistant_data:\n",
        "                 print(f\"      Failed to get current assistant data (Status: {get_status}). Response:\")\n",
        "                 print(json.dumps(current_assistant_data, indent=2))\n",
        "                 raise Exception(\"Failed to get assistant data for update via REST.\")\n",
        "\n",
        "            # Form payload for update\n",
        "            update_payload = {\n",
        "                \"model\": current_assistant_data.get(\"model\"), # Need to pass model\n",
        "                \"instructions\": current_assistant_data.get(\"instructions\"),\n",
        "                \"name\": current_assistant_data.get(\"name\"),\n",
        "                \"tools\": current_assistant_data.get(\"tools\", []), # Take current tools\n",
        "                \"tool_resources\": tool_resources_payload # Add our resources\n",
        "            }\n",
        "            # Add file_search to tools if not present\n",
        "            if not any(tool.get('type') == 'file_search' for tool in update_payload[\"tools\"]):\n",
        "                 update_payload[\"tools\"].append({\"type\": \"file_search\"})\n",
        "                 print(\"      'file_search' tool added to payload.\")\n",
        "\n",
        "            print(\"      Sending assistant update request via REST...\")\n",
        "            update_status, update_response_data = rest_api_request('POST', assistant_update_url, api_key, payload=update_payload)\n",
        "\n",
        "            if update_status == 200:\n",
        "                print(f\"   Assistant '{update_response_data.get('name', ASSISTANT_ID)}' updated successfully via REST API!\")\n",
        "            else:\n",
        "                print(f\"   Error updating assistant via REST API (Status: {update_status}). Response:\")\n",
        "                print(json.dumps(update_response_data, indent=2))\n",
        "                raise Exception(\"Failed to update assistant via REST API.\")\n",
        "\n",
        "        # --- Final Message --- #\n",
        "        print(f\"\\nSUCCESS! File Search (Retrieval) configured via Vector Store {vector_store_id} using REST API.\")\n",
        "        print(\"\\nYou can now return to cell 6 and test File Search.\")\n",
        "        print(\"Try asking: 'What vegetables are in season in Ireland in May?'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during File Search setup: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        # --- Attempt resource cleanup on error --- #\n",
        "        print(\"\\n--- Attempting resource cleanup --- \")\n",
        "        if vector_store_id:\n",
        "            try:\n",
        "                print(f\"   Deleting Vector Store {vector_store_id}...\")\n",
        "                delete_vs_url = f\"https://api.openai.com/v1/vector_stores/{vector_store_id}\"\n",
        "                status, data = rest_api_request('DELETE', delete_vs_url, api_key)\n",
        "                if status == 200 and data and data.get('deleted'):\n",
        "                    print(f\"      Vector Store {vector_store_id} deleted.\")\n",
        "                else:\n",
        "                    print(f\"      Failed to delete Vector Store {vector_store_id} (Status: {status}). Response: {data}\")\n",
        "            except Exception as delete_vs_err:\n",
        "                print(f\"      Error deleting Vector Store: {delete_vs_err}\")\n",
        "        # Delete file uploaded via SDK using SDK\n",
        "        if uploaded_file_id and local_client:\n",
        "            try:\n",
        "                print(f\"   Deleting file {uploaded_file_id}...\")\n",
        "                local_client.files.delete(uploaded_file_id)\n",
        "                print(f\"      File {uploaded_file_id} deleted.\")\n",
        "            except Exception as delete_file_err:\n",
        "                print(f\"      Error deleting file: {delete_file_err}\")\n",
        "        elif uploaded_file_id:\n",
        "             print(f\"   Failed to delete file {uploaded_file_id} (SDK client not initialized).\")\n",
        "\n",
        "# --- Run Setup --- #\n",
        "setup_file_search_via_rest()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tewpn8Z-9f5m",
        "outputId": "ce9aeffb-a84a-45e2-ba7a-bf9753289979"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using OpenAI version: 1.76.0\n",
            "File for RAG '/home/ubuntu/cooking_tips_rag.txt' created/overwritten.\n",
            "Using Assistant ID from cell 5: asst_36JdysTVWsHUJHfz3DzIXloq\n",
            "Creating OpenAI client (for file upload)...\n",
            "Client created.\n",
            "1. Uploading file /home/ubuntu/cooking_tips_rag.txt via SDK...\n",
            "   File uploaded successfully, File ID: file-BDDUEth3tHeLqBMgWPpadc\n",
            "2. Creating Vector Store 'Seasonal Produce Ireland Store REST' via REST API...\n",
            "   Vector Store created, ID: vs_6817b26278d48191875d10b3c67c3853\n",
            "3. Adding file file-BDDUEth3tHeLqBMgWPpadc to Vector Store vs_6817b26278d48191875d10b3c67c3853 via REST API...\n",
            "   File file-BDDUEth3tHeLqBMgWPpadc added to Vector Store. Status: in_progress\n",
            "4. Waiting for file file-BDDUEth3tHeLqBMgWPpadc processing in Vector Store (status 'completed')...\n",
            "   Current status of file in VS: completed\n",
            "   File processed successfully in Vector Store.\n",
            "5. Updating assistant asst_36JdysTVWsHUJHfz3DzIXloq to use File Search and Vector Store vs_6817b26278d48191875d10b3c67c3853...\n",
            "   Attempting update via SDK...\n",
            "      'file_search' tool added.\n",
            "   Assistant 'Weather Recipe Guru v2' updated successfully via SDK!\n",
            "\n",
            "SUCCESS! File Search (Retrieval) configured via Vector Store vs_6817b26278d48191875d10b3c67c3853 using REST API.\n",
            "\n",
            "You can now return to cell 6 and test File Search.\n",
            "Try asking: 'What vegetables are in season in Ireland in May?'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "print(\"OpenAI version:\", openai.__version__)\n",
        "\n",
        "\n",
        "try:\n",
        "    from openai.resources import assistants_files\n",
        "except ImportError as e:\n",
        "    print(\" ImportError:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "U0n_-ebLQi51",
        "outputId": "1759c628-878b-4226-ef8c-00b87ba3d6ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI version: 1.76.0\n",
            " ImportError: cannot import name 'assistants_files' from 'openai.resources' (/usr/local/lib/python3.11/dist-packages/openai/resources/__init__.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update && sudo apt-get install -y sqlite3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7xJyW0ITiBVi",
        "outputId": "2bfe22ed-9478-4f38-8f93-5e04d897b960"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [77.5 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,715 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,244 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,544 kB]\n",
            "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,659 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,910 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,420 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,200 kB]\n",
            "Fetched 24.2 MB in 3s (7,651 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  sqlite3-doc\n",
            "The following NEW packages will be installed:\n",
            "  sqlite3\n",
            "0 upgraded, 1 newly installed, 0 to remove and 43 not upgraded.\n",
            "Need to get 768 kB of archives.\n",
            "After this operation, 1,873 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sqlite3 amd64 3.37.2-2ubuntu0.3 [768 kB]\n",
            "Fetched 768 kB in 1s (599 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package sqlite3.\n",
            "(Reading database ... 126101 files and directories currently installed.)\n",
            "Preparing to unpack .../sqlite3_3.37.2-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking sqlite3 (3.37.2-2ubuntu0.3) ...\n",
            "Setting up sqlite3 (3.37.2-2ubuntu0.3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sqlite3 -header -column \"/content/drive/MyDrive/ColabData/recipe_assistant_db.sqlite\" \"SELECT * FROM user_profiles;\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "AXAvIIgfqb2v",
        "outputId": "5525b3fc-1271-4853-c305-cf9675717780"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_id  preferences_json                               allergies_json\n",
            "-------  ---------------------------------------------  --------------\n",
            "Arsenii  {\"likes\": [\"italian\"], \"dislikes\": [\"spicy\"]}  [\"shellfish\"] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update && apt-get install -y sqlite3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE1tpQLQgep2",
        "outputId": "2241f24a-411e-4a2c-b1d5-52470e5df94f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.83)] [\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r0% [2 InRelease 28.7 kB/128 kB 22%] [Waiting for headers] [Waiting for headers]\r                                                                               \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [2 InRelease 53.3 kB/128 kB 42%] [Waiting for headers] [3 InRelease 3,632 B/\r                                                                               \rGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\r0% [2 InRelease 57.6 kB/128 kB 45%] [Waiting for headers] [3 InRelease 3,632 B/\r0% [2 InRelease 57.6 kB/128 kB 45%] [Waiting for headers] [3 InRelease 3,632 B/\r0% [2 InRelease 57.6 kB/128 kB 45%] [Waiting for headers] [Connected to r2u.sta\r                                                                               \rGet:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [2 InRelease 128 kB/128 kB 100%] [5 InRelease 14.2 kB/129 kB 11%] [Waiting f\r0% [Waiting for headers] [5 InRelease 14.2 kB/129 kB 11%] [Waiting for headers]\r                                                                               \rGet:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,659 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,200 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,420 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,544 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,718 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,907 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,244 kB]\n",
            "Fetched 24.1 MB in 4s (5,957 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  sqlite3-doc\n",
            "The following NEW packages will be installed:\n",
            "  sqlite3\n",
            "0 upgraded, 1 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 768 kB of archives.\n",
            "After this operation, 1,873 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sqlite3 amd64 3.37.2-2ubuntu0.3 [768 kB]\n",
            "Fetched 768 kB in 0s (3,874 kB/s)\n",
            "Selecting previously unselected package sqlite3.\n",
            "(Reading database ... 126101 files and directories currently installed.)\n",
            "Preparing to unpack .../sqlite3_3.37.2-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking sqlite3 (3.37.2-2ubuntu0.3) ...\n",
            "Setting up sqlite3 (3.37.2-2ubuntu0.3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Gradio Chat Interface Cell (Fixed v7) ===\n",
        "\n",
        "print(\"Installing Gradio...\")\n",
        "!pip install gradio -q\n",
        "\n",
        "print(\"Importing necessary modules for Gradio UI...\")\n",
        "import gradio as gr\n",
        "import openai\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import requests\n",
        "import sqlite3 # Needed for profile functions\n",
        "\n",
        "# --- Ensure previous cells have run ---\n",
        "# Check for necessary variables from previous cells\n",
        "required_vars = [\n",
        "    'client', 'ASSISTANT_ID', 'OPENAI_API_KEY',\n",
        "    'SPOONACULAR_API_KEY', 'OPENWEATHERMAP_API_KEY',\n",
        "    'DB_PATH' # From cell 2/3 for SQLite\n",
        "]\n",
        "missing_vars = [var for var in required_vars if var not in globals()]\n",
        "\n",
        "if missing_vars:\n",
        "    print(f\"\\nERROR: The following required variables are missing: {', '.join(missing_vars)}\")\n",
        "    print(\"Please ensure you have run the previous cells (especially 2, 3, 5, and 19 if using File Search) successfully.\")\n",
        "    # Stop execution if prerequisites are not met\n",
        "    raise NameError(f\"Missing required variables: {', '.join(missing_vars)}\")\n",
        "else:\n",
        "    print(\"\\nAll necessary variables found. Proceeding with Gradio setup.\")\n",
        "\n",
        "# --- Re-define necessary helper functions (or ensure they are globally accessible) ---\n",
        "# Assuming functions from cells 3 and 6 (get_db_connection, init_db, get_user_profile, etc.,\n",
        "# get_weather, find_recipes, get_recipe_details) are available globally.\n",
        "\n",
        "# Make sure DB is initialized (redundant if cell 3 ran, but safe)\n",
        "try:\n",
        "    init_db()\n",
        "except NameError:\n",
        "    print(\"Warning: init_db() function not found globally. Assuming DB is initialized.\")\n",
        "except Exception as db_init_err:\n",
        "    print(f\"Warning: Error during redundant DB initialization: {db_init_err}\")\n",
        "\n",
        "# Dictionary mapping function names to implementations (must be globally accessible)\n",
        "available_functions = {\n",
        "    \"get_weather\": get_weather,\n",
        "    \"find_recipes\": find_recipes,\n",
        "    \"get_recipe_details\": get_recipe_details,\n",
        "    \"get_user_profile\": get_user_profile,\n",
        "    \"save_user_preference\": save_user_preference,\n",
        "    \"save_user_allergy\": save_user_allergy,\n",
        "}\n",
        "\n",
        "# --- Core Logic for Gradio Chatbot ---\n",
        "\n",
        "def process_run(thread_id, run_id):\n",
        "    \"\"\"Processes a run, handles actions, and returns the final assistant message string.\"\"\"\n",
        "    while True:\n",
        "        run = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run_id)\n",
        "        print(f\"[Gradio Backend] Run Status: {run.status}\") # Debug output\n",
        "\n",
        "        if run.status == 'completed':\n",
        "            messages = client.beta.threads.messages.list(thread_id=thread_id, order=\"desc\", limit=1) # Get latest message\n",
        "            assistant_message = \"\" # Initialize\n",
        "            if messages.data and messages.data[0].role == \"assistant\":\n",
        "                for content_block in messages.data[0].content:\n",
        "                    if content_block.type == 'text':\n",
        "                        assistant_message += content_block.text.value + \"\\n\"\n",
        "            return assistant_message.strip() if assistant_message else \"Assistant did not provide a text response.\"\n",
        "\n",
        "        elif run.status == 'requires_action':\n",
        "            print(\"[Gradio Backend] Assistant requires function calls...\")\n",
        "            tool_outputs = []\n",
        "            for tool_call in run.required_action.submit_tool_outputs.tool_calls:\n",
        "                function_name = tool_call.function.name\n",
        "                try:\n",
        "                    function_args = json.loads(tool_call.function.arguments)\n",
        "                except json.JSONDecodeError:\n",
        "                     print(f\"[Gradio Backend] Error: Invalid JSON arguments from assistant for {function_name}: {tool_call.function.arguments}\")\n",
        "                     tool_outputs.append({\"tool_call_id\": tool_call.id, \"output\": json.dumps({\"error\": \"Invalid JSON arguments received\"})})\n",
        "                     continue # Skip this tool call\n",
        "\n",
        "                print(f\"  - Calling function: {function_name}\")\n",
        "                print(f\"    Arguments: {function_args}\")\n",
        "\n",
        "                if function_name in available_functions:\n",
        "                    function_to_call = available_functions[function_name]\n",
        "                    try:\n",
        "                        function_response = function_to_call(**function_args)\n",
        "                        output_str = str(function_response)\n",
        "                        print(f\"    Result: {output_str[:200]}...\" if len(output_str) > 200 else f\"    Result: {output_str}\")\n",
        "                        tool_outputs.append({\"tool_call_id\": tool_call.id, \"output\": output_str})\n",
        "                    except Exception as e:\n",
        "                        print(f\"    Error executing function {function_name}: {e}\")\n",
        "                        tool_outputs.append({\"tool_call_id\": tool_call.id, \"output\": json.dumps({\"error\": f\"Error in {function_name}: {e}\"})})\n",
        "                else:\n",
        "                    print(f\"    Error: Function '{function_name}' not found!\")\n",
        "                    tool_outputs.append({\"tool_call_id\": tool_call.id, \"output\": json.dumps({\"error\": f\"Function '{function_name}' not defined.\"})})\n",
        "\n",
        "            if tool_outputs:\n",
        "                try:\n",
        "                    client.beta.threads.runs.submit_tool_outputs(\n",
        "                        thread_id=thread_id,\n",
        "                        run_id=run.id,\n",
        "                        tool_outputs=tool_outputs\n",
        "                    )\n",
        "                    print(\"[Gradio Backend] Submitted function results.\")\n",
        "                    # Continue the loop to wait for the next status\n",
        "                except Exception as e:\n",
        "                    print(f\"[Gradio Backend] Error submitting tool outputs: {e}\")\n",
        "                    return f\"Error submitting function results: {e}\" # Return error message\n",
        "            else:\n",
        "                print(\"[Gradio Backend] No tool outputs generated despite requires_action status.\")\n",
        "                return \"Error: Assistant required action but no function calls were processed.\" # Return error message\n",
        "\n",
        "        elif run.status in ['queued', 'in_progress', 'cancelling']:\n",
        "            time.sleep(1) # Wait before checking again\n",
        "\n",
        "        elif run.status in ['failed', 'cancelled', 'expired']:\n",
        "            error_message = f\"Run {run.status.upper()}.\"\n",
        "            if run.last_error:\n",
        "                error_message += f\" Reason: {run.last_error.message}\"\n",
        "            print(f\"[Gradio Backend] {error_message}\")\n",
        "            return error_message # Return error message\n",
        "        else:\n",
        "            print(f\"[Gradio Backend] Unexpected run status: {run.status}\")\n",
        "            return f\"Unexpected error: Run status is {run.status}\" # Return error message\n",
        "\n",
        "def assistant_chat(message, history, thread_id_state):\n",
        "    \"\"\"Handles a single turn of the chat. Returns tuple: (updated_thread_id, assistant_response_string).\"\"\"\n",
        "    print(f\"\\n[Gradio Backend] Received message: '{message}'\")\n",
        "    print(f\"[Gradio Backend] Current Thread ID State: {thread_id_state}\")\n",
        "    # History is managed by Gradio ChatInterface itself when type=\"messages\", we don't need to process it here\n",
        "\n",
        "    # Get or create thread ID\n",
        "    thread_id = thread_id_state\n",
        "    if not thread_id:\n",
        "        try:\n",
        "            thread = client.beta.threads.create()\n",
        "            thread_id = thread.id\n",
        "            print(f\"[Gradio Backend] New thread created: {thread_id}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[Gradio Backend] Error creating thread: {e}\")\n",
        "            # Return current state and error message string\n",
        "            return thread_id_state, f\"Error creating conversation thread: {e}\"\n",
        "\n",
        "    # Add user message to the thread\n",
        "    try:\n",
        "        client.beta.threads.messages.create(\n",
        "            thread_id=thread_id,\n",
        "            role=\"user\",\n",
        "            content=message\n",
        "        )\n",
        "        print(f\"[Gradio Backend] User message added to thread {thread_id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[Gradio Backend] Error adding message to thread: {e}\")\n",
        "        # Return updated thread_id and error message string\n",
        "        return thread_id, f\"Error sending message: {e}\"\n",
        "\n",
        "    # Run the assistant\n",
        "    try:\n",
        "        run = client.beta.threads.runs.create(\n",
        "            thread_id=thread_id,\n",
        "            assistant_id=ASSISTANT_ID\n",
        "        )\n",
        "        print(f\"[Gradio Backend] Assistant run created: {run.id}\")\n",
        "\n",
        "        # Process the run and get the final response string\n",
        "        assistant_response = process_run(thread_id, run.id)\n",
        "        print(f\"[Gradio Backend] Assistant response string: {assistant_response}\")\n",
        "\n",
        "        # Return the updated thread_id and the assistant's response string\n",
        "        return thread_id, assistant_response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[Gradio Backend] Error running assistant or processing run: {e}\")\n",
        "        # Return updated thread_id and error message string\n",
        "        return thread_id, f\"Error during assistant processing: {e}\"\n",
        "\n",
        "# --- Setup Gradio Interface --- #\n",
        "print(\"\\nSetting up Gradio Chat Interface...\")\n",
        "\n",
        "# Use gr.State to manage the thread_id across calls within a session\n",
        "thread_state = gr.State(None)\n",
        "\n",
        "# Wrapper function for Gradio ChatInterface\n",
        "def stateful_chat_wrapper(message, history, current_thread_id):\n",
        "    \"\"\"Takes message, history, and state. Returns tuple: (assistant_response_string, updated_thread_id).\"\"\"\n",
        "    # If history is None (first turn), initialize it as an empty list\n",
        "    if history is None:\n",
        "        history = []\n",
        "    print(f\"[Gradio Backend] History received (type={type(history)}): {history}\") # Debug history\n",
        "\n",
        "    # Call the main logic function which returns (updated_thread_id, response_string)\n",
        "    final_thread_id, final_response = assistant_chat(message, history, current_thread_id)\n",
        "\n",
        "    # *** IMPORTANT: Return the response string AND the updated state ***\n",
        "    # The history is automatically updated by ChatInterface when type=\"messages\".\n",
        "    # The first return value goes to the chatbot output.\n",
        "    # The second return value goes to the state output (additional_outputs[0]).\n",
        "    print(f\"[Gradio Backend] Returning response string: {final_response}\")\n",
        "    print(f\"[Gradio Backend] Returning final thread ID: {final_thread_id}\")\n",
        "\n",
        "    return final_response, final_thread_id\n",
        "\n",
        "\n",
        "# Define the Gradio components explicitly for clarity\n",
        "# *** CHANGE: Explicitly set type=\"messages\" ***\n",
        "chatbot = gr.Chatbot(label=\"Weather Recipe Guru\", height=600, type=\"messages\")\n",
        "textbox = gr.Textbox(placeholder=\"Ask me for recipe ideas based on the weather!\", container=False, scale=7)\n",
        "\n",
        "# Create the ChatInterface\n",
        "iface = gr.ChatInterface(\n",
        "    fn=stateful_chat_wrapper,\n",
        "    chatbot=chatbot,\n",
        "    textbox=textbox,\n",
        "    title=\"Weather Recipe Guru Assistant\",\n",
        "    description=\"Chat with the assistant to get recipe suggestions based on weather, preferences, and allergies.\",\n",
        "    theme=\"soft\",\n",
        "    examples=[[\"Hello\"], [\"What can I make for dinner in London?\"], [\"Suggest a vegetarian soup, I don't like mushrooms\"]],\n",
        "    cache_examples=False,\n",
        "    # Pass the state variable as an additional input and output\n",
        "    # Inputs: message (from textbox), history (from chatbot), current_thread_id (from state)\n",
        "    # Outputs:\n",
        "    # 1. The first return value of fn (final_response) goes to chatbot.\n",
        "    # 2. The second return value of fn (final_thread_id) goes to additional_outputs[0] (thread_state).\n",
        "    additional_inputs=[thread_state],\n",
        "    additional_outputs=[thread_state]\n",
        ")\n",
        "\n",
        "# --- Launch Gradio Interface --- #\n",
        "print(\"\\nLaunching Gradio Interface...\")\n",
        "iface.launch(inline=True, debug=True) # debug=True shows more logs\n",
        "\n",
        "print(\"\\nGradio Interface launched. Interact with the chat window above.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4516
        },
        "id": "InYYF_VTaNOs",
        "outputId": "9bfa2069-ca0a-42e7-f29e-7ba8798d011e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing Gradio...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hImporting necessary modules for Gradio UI...\n",
            "\n",
            "All necessary variables found. Proceeding with Gradio setup.\n",
            "Database initialized successfully.\n",
            "\n",
            "Setting up Gradio Chat Interface...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:321: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'messages', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Launching Gradio Interface...\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a1f9cb72f470ea7a0b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a1f9cb72f470ea7a0b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gradio Backend] History received (type=<class 'list'>): []\n",
            "\n",
            "[Gradio Backend] Received message: 'Hello'\n",
            "[Gradio Backend] Current Thread ID State: None\n",
            "[Gradio Backend] New thread created: thread_GTDiTiMrgYV9370CqFkyjyJB\n",
            "[Gradio Backend] User message added to thread thread_GTDiTiMrgYV9370CqFkyjyJB\n",
            "[Gradio Backend] Assistant run created: run_w9826n0D4axBTX1RbDHMbMio\n",
            "[Gradio Backend] Run Status: queued\n",
            "[Gradio Backend] Run Status: in_progress\n",
            "[Gradio Backend] Run Status: completed\n",
            "[Gradio Backend] Assistant response string: Hi there! How can I assist you today? If you don't mind sharing, what's your name and which city are you in? This will help me tailor my suggestions for you!\n",
            "[Gradio Backend] Returning response string: Hi there! How can I assist you today? If you don't mind sharing, what's your name and which city are you in? This will help me tailor my suggestions for you!\n",
            "[Gradio Backend] Returning final thread ID: thread_GTDiTiMrgYV9370CqFkyjyJB\n",
            "[Gradio Backend] History received (type=<class 'list'>): [{'role': 'user', 'metadata': None, 'content': 'Hello', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': \"Hi there! How can I assist you today? If you don't mind sharing, what's your name and which city are you in? This will help me tailor my suggestions for you!\", 'options': None}]\n",
            "\n",
            "[Gradio Backend] Received message: 'Im Arsenii from Dublin'\n",
            "[Gradio Backend] Current Thread ID State: thread_GTDiTiMrgYV9370CqFkyjyJB\n",
            "[Gradio Backend] User message added to thread thread_GTDiTiMrgYV9370CqFkyjyJB\n",
            "[Gradio Backend] Assistant run created: run_JOiqqF2uW2CXL4f5Ov4xuJB9\n",
            "[Gradio Backend] Run Status: queued\n",
            "[Gradio Backend] Run Status: in_progress\n",
            "[Gradio Backend] Run Status: requires_action\n",
            "[Gradio Backend] Assistant requires function calls...\n",
            "  - Calling function: get_user_profile\n",
            "    Arguments: {'user_id': 'Arsenii'}\n",
            "    Result: {\"user_id\": \"Arsenii\", \"preferences\": {\"likes\": [\"italian\"], \"dislikes\": [\"spicy\"]}, \"allergies\": [\"shellfish\"]}\n",
            "  - Calling function: get_weather\n",
            "    Arguments: {'city': 'Dublin'}\n",
            "\n",
            "[Function] Requesting weather for city: Dublin\n",
            "[Function] Weather received: {'temperature': 17.3, 'description': 'clear sky'}\n",
            "    Result: {\"temperature\": 17.3, \"description\": \"clear sky\"}\n",
            "[Gradio Backend] Submitted function results.\n",
            "[Gradio Backend] Run Status: queued\n",
            "[Gradio Backend] Run Status: in_progress\n",
            "[Gradio Backend] Run Status: completed\n",
            "[Gradio Backend] Assistant response string: Great to meet you, Arsenii! It's a lovely 17.3°C with clear skies in Dublin today. I see that you enjoy Italian cuisine and prefer to avoid spicy dishes, and you're allergic to shellfish. \n",
            "\n",
            "What kind of meal are you in the mood for today? Are you looking for something quick, or do you have a specific dish in mind?\n",
            "[Gradio Backend] Returning response string: Great to meet you, Arsenii! It's a lovely 17.3°C with clear skies in Dublin today. I see that you enjoy Italian cuisine and prefer to avoid spicy dishes, and you're allergic to shellfish. \n",
            "\n",
            "What kind of meal are you in the mood for today? Are you looking for something quick, or do you have a specific dish in mind?\n",
            "[Gradio Backend] Returning final thread ID: thread_GTDiTiMrgYV9370CqFkyjyJB\n",
            "[Gradio Backend] History received (type=<class 'list'>): [{'role': 'user', 'metadata': None, 'content': 'Hello', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': \"Hi there! How can I assist you today? If you don't mind sharing, what's your name and which city are you in? This will help me tailor my suggestions for you!\", 'options': None}, {'role': 'user', 'metadata': None, 'content': 'Im Arsenii from Dublin', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': \"Great to meet you, Arsenii! It's a lovely 17.3°C with clear skies in Dublin today. I see that you enjoy Italian cuisine and prefer to avoid spicy dishes, and you're allergic to shellfish. \\n\\nWhat kind of meal are you in the mood for today? Are you looking for something quick, or do you have a specific dish in mind?\", 'options': None}]\n",
            "\n",
            "[Gradio Backend] Received message: 'Something fast to prepare'\n",
            "[Gradio Backend] Current Thread ID State: thread_GTDiTiMrgYV9370CqFkyjyJB\n",
            "[Gradio Backend] User message added to thread thread_GTDiTiMrgYV9370CqFkyjyJB\n",
            "[Gradio Backend] Assistant run created: run_1zXTuoIwdOrOn3XJvlRltkAC\n",
            "[Gradio Backend] Run Status: queued\n",
            "[Gradio Backend] Run Status: in_progress\n",
            "[Gradio Backend] Run Status: in_progress\n",
            "[Gradio Backend] Run Status: requires_action\n",
            "[Gradio Backend] Assistant requires function calls...\n",
            "  - Calling function: find_recipes\n",
            "    Arguments: {'weather_description': 'clear sky', 'temperature': 17.3, 'user_query': 'fast preparation', 'cuisine': 'italian', 'intolerances': ['shellfish'], 'exclude_ingredients': ['spicy']}\n",
            "\n",
            "[Function] Searching recipes. Weather: clear sky, 17.3°C. Query: fast preparation\n",
            "[Function] Parameters for Spoonacular request: {'apiKey': '311f06078d6540ffad7b9a100103add0', 'query': 'fast preparation', 'cuisine': 'italian', 'intolerances': 'shellfish', 'excludeIngredients': 'spicy', 'number': 3, 'addRecipeInformation': True}\n",
            "[Function] Recipes found: 0\n",
            "[Function] No recipes found for the given criteria.\n",
            "    Result: {\"message\": \"Unfortunately, nothing was found for your request. Try changing the criteria.\"}\n",
            "[Gradio Backend] Submitted function results.\n",
            "[Gradio Backend] Run Status: queued\n",
            "[Gradio Backend] Run Status: in_progress\n",
            "[Gradio Backend] Run Status: completed\n",
            "[Gradio Backend] Assistant response string: It seems that I couldn't find any fast Italian recipes that match your preferences and restrictions right now. How about we broaden the search a bit? Would you be open to other types of cuisine, or perhaps using different ingredients? Let me know your thoughts!\n",
            "[Gradio Backend] Returning response string: It seems that I couldn't find any fast Italian recipes that match your preferences and restrictions right now. How about we broaden the search a bit? Would you be open to other types of cuisine, or perhaps using different ingredients? Let me know your thoughts!\n",
            "[Gradio Backend] Returning final thread ID: thread_GTDiTiMrgYV9370CqFkyjyJB\n",
            "[Gradio Backend] History received (type=<class 'list'>): [{'role': 'user', 'metadata': None, 'content': 'Hello', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': \"Hi there! How can I assist you today? If you don't mind sharing, what's your name and which city are you in? This will help me tailor my suggestions for you!\", 'options': None}, {'role': 'user', 'metadata': None, 'content': 'Im Arsenii from Dublin', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': \"Great to meet you, Arsenii! It's a lovely 17.3°C with clear skies in Dublin today. I see that you enjoy Italian cuisine and prefer to avoid spicy dishes, and you're allergic to shellfish. \\n\\nWhat kind of meal are you in the mood for today? Are you looking for something quick, or do you have a specific dish in mind?\", 'options': None}, {'role': 'user', 'metadata': None, 'content': 'Something fast to prepare', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': \"It seems that I couldn't find any fast Italian recipes that match your preferences and restrictions right now. How about we broaden the search a bit? Would you be open to other types of cuisine, or perhaps using different ingredients? Let me know your thoughts!\", 'options': None}]\n",
            "\n",
            "[Gradio Backend] Received message: 'Yeah, suggest me something for the weather around me. i wanna go for picknic '\n",
            "[Gradio Backend] Current Thread ID State: thread_GTDiTiMrgYV9370CqFkyjyJB\n",
            "[Gradio Backend] User message added to thread thread_GTDiTiMrgYV9370CqFkyjyJB\n",
            "[Gradio Backend] Assistant run created: run_HGz3j6ZiUmkbSjsXnwDZfyFp\n",
            "[Gradio Backend] Run Status: queued\n",
            "[Gradio Backend] Run Status: in_progress\n",
            "[Gradio Backend] Run Status: requires_action\n",
            "[Gradio Backend] Assistant requires function calls...\n",
            "  - Calling function: find_recipes\n",
            "    Arguments: {'weather_description': 'clear sky', 'temperature': 17.3, 'user_query': 'picnic', 'intolerances': ['shellfish'], 'exclude_ingredients': ['spicy']}\n",
            "\n",
            "[Function] Searching recipes. Weather: clear sky, 17.3°C. Query: picnic\n",
            "[Function] Parameters for Spoonacular request: {'apiKey': '311f06078d6540ffad7b9a100103add0', 'query': 'picnic', 'intolerances': 'shellfish', 'excludeIngredients': 'spicy', 'number': 3, 'addRecipeInformation': True}\n",
            "[Function] Recipes found: 2\n",
            "    Result: [{\"id\": 1096307, \"title\": \"Pine Nut and Cranberry Picnic Chicken Salad\", \"image\": \"https://img.spoonacular.com/recipes/1096307-312x231.jpeg\", \"readyInMinutes\": 30}, {\"id\": 661249, \"title\": \"Spinach & ...\n",
            "[Gradio Backend] Submitted function results.\n",
            "[Gradio Backend] Run Status: queued\n",
            "[Gradio Backend] Run Status: in_progress\n",
            "[Gradio Backend] Run Status: in_progress\n",
            "[Gradio Backend] Run Status: in_progress\n",
            "[Gradio Backend] Run Status: completed\n",
            "[Gradio Backend] Assistant response string: Here are a couple of delicious picnic recipes that are perfect for the lovely weather in Dublin:\n",
            "\n",
            "1. **Pine Nut and Cranberry Picnic Chicken Salad**\n",
            "   - ![Pine Nut and Cranberry Picnic Chicken Salad](https://img.spoonacular.com/recipes/1096307-312x231.jpeg)\n",
            "   - Ready in: 30 minutes\n",
            "\n",
            "2. **Spinach & Ham Quiche**\n",
            "   - ![Spinach & Ham Quiche](https://img.spoonacular.com/recipes/661249-312x231.jpg)\n",
            "   - Ready in: 45 minutes\n",
            "\n",
            "Do either of these sound good to you? If you choose one, I can provide the detailed recipe!\n",
            "[Gradio Backend] Returning response string: Here are a couple of delicious picnic recipes that are perfect for the lovely weather in Dublin:\n",
            "\n",
            "1. **Pine Nut and Cranberry Picnic Chicken Salad**\n",
            "   - ![Pine Nut and Cranberry Picnic Chicken Salad](https://img.spoonacular.com/recipes/1096307-312x231.jpeg)\n",
            "   - Ready in: 30 minutes\n",
            "\n",
            "2. **Spinach & Ham Quiche**\n",
            "   - ![Spinach & Ham Quiche](https://img.spoonacular.com/recipes/661249-312x231.jpg)\n",
            "   - Ready in: 45 minutes\n",
            "\n",
            "Do either of these sound good to you? If you choose one, I can provide the detailed recipe!\n",
            "[Gradio Backend] Returning final thread ID: thread_GTDiTiMrgYV9370CqFkyjyJB\n",
            "[Gradio Backend] History received (type=<class 'list'>): [{'role': 'user', 'metadata': None, 'content': 'Hello', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': \"Hi there! How can I assist you today? If you don't mind sharing, what's your name and which city are you in? This will help me tailor my suggestions for you!\", 'options': None}, {'role': 'user', 'metadata': None, 'content': 'Im Arsenii from Dublin', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': \"Great to meet you, Arsenii! It's a lovely 17.3°C with clear skies in Dublin today. I see that you enjoy Italian cuisine and prefer to avoid spicy dishes, and you're allergic to shellfish. \\n\\nWhat kind of meal are you in the mood for today? Are you looking for something quick, or do you have a specific dish in mind?\", 'options': None}, {'role': 'user', 'metadata': None, 'content': 'Something fast to prepare', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': \"It seems that I couldn't find any fast Italian recipes that match your preferences and restrictions right now. How about we broaden the search a bit? Would you be open to other types of cuisine, or perhaps using different ingredients? Let me know your thoughts!\", 'options': None}, {'role': 'user', 'metadata': None, 'content': 'Yeah, suggest me something for the weather around me. i wanna go for picknic ', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Here are a couple of delicious picnic recipes that are perfect for the lovely weather in Dublin:\\n\\n1. **Pine Nut and Cranberry Picnic Chicken Salad**\\n   - ![Pine Nut and Cranberry Picnic Chicken Salad](https://img.spoonacular.com/recipes/1096307-312x231.jpeg)\\n   - Ready in: 30 minutes\\n\\n2. **Spinach & Ham Quiche**\\n   - ![Spinach & Ham Quiche](https://img.spoonacular.com/recipes/661249-312x231.jpg)\\n   - Ready in: 45 minutes\\n\\nDo either of these sound good to you? If you choose one, I can provide the detailed recipe!', 'options': None}]\n",
            "\n",
            "[Gradio Backend] Received message: 'i like the 2nd one, i like spinach'\n",
            "[Gradio Backend] Current Thread ID State: thread_GTDiTiMrgYV9370CqFkyjyJB\n",
            "[Gradio Backend] User message added to thread thread_GTDiTiMrgYV9370CqFkyjyJB\n",
            "[Gradio Backend] Assistant run created: run_mMaz6F9Xqp1WTi8wrKQOi2zJ\n",
            "[Gradio Backend] Run Status: queued\n",
            "[Gradio Backend] Run Status: in_progress\n",
            "[Gradio Backend] Run Status: requires_action\n",
            "[Gradio Backend] Assistant requires function calls...\n",
            "  - Calling function: get_recipe_details\n",
            "    Arguments: {'recipe_id': 661249}\n",
            "\n",
            "[Function] Requesting details for recipe ID: 661249\n",
            "[Function] Details for recipe 'Spinach & Ham Quiche' received.\n",
            "    Result: {\"title\": \"Spinach & Ham Quiche\", \"readyInMinutes\": 45, \"servings\": 8, \"ingredients\": [\"100 grams Cold butter\", \"50 grams Cheddar cheese- grated\", \"2 Large eggs\", \"125 grams Chicken or Picnic Ham (shr...\n",
            "[Gradio Backend] Submitted function results.\n",
            "[Gradio Backend] Run Status: queued\n",
            "[Gradio Backend] Run Status: in_progress\n",
            "[Gradio Backend] Run Status: in_progress\n",
            "[Gradio Backend] Run Status: in_progress\n",
            "[Gradio Backend] Run Status: in_progress\n",
            "[Gradio Backend] Run Status: completed\n",
            "[Gradio Backend] Assistant response string: Here's the recipe for **Spinach & Ham Quiche**! It's perfect for a picnic and delicious. \n",
            "\n",
            "### Ingredients:\n",
            "- 100 grams Cold butter\n",
            "- 50 grams Cheddar cheese (grated)\n",
            "- 2 Large eggs\n",
            "- 125 grams Chicken or Picnic Ham (shredded)\n",
            "- 5 tablespoons Ice water\n",
            "- 80 milliliters milk\n",
            "- Dash of white pepper\n",
            "- 200 grams Plain flour\n",
            "- 1/8 teaspoon salt\n",
            "- 1/2 tablespoon Shallots (sliced)\n",
            "- 125 grams Spinach (wash & drain well, shredded)\n",
            "- 80 milliliters Whipping cream\n",
            "\n",
            "### Instructions:\n",
            "1. With a little oil, fry the shallot till fragrant and golden brown.\n",
            "2. Add the ham and fry for a while, then add the spinach and fry until the spinach is cooked. Dish up and set aside.\n",
            "3. Measure the whipping cream and milk together in a measuring cup (you can omit whipping cream and use all 160ml milk). Add eggs (lightly beaten) and a pinch of salt, then mix well.\n",
            "4. Spread the ham, spinach, and cheddar cheese evenly over the bottom of the prepared tart pan. Add a dash of pepper, then gently pour the egg mixture over it.\n",
            "5. Bake in a preheated oven at 180°C for 40 minutes and serve hot or warm.\n",
            "\n",
            "You can find more details [here](https://www.foodista.com/recipe/Z7KLXBR8/spinach-ham-quiche).\n",
            "\n",
            "Enjoy your picnic, Arsenii! If you need anything else, just let me know!\n",
            "[Gradio Backend] Returning response string: Here's the recipe for **Spinach & Ham Quiche**! It's perfect for a picnic and delicious. \n",
            "\n",
            "### Ingredients:\n",
            "- 100 grams Cold butter\n",
            "- 50 grams Cheddar cheese (grated)\n",
            "- 2 Large eggs\n",
            "- 125 grams Chicken or Picnic Ham (shredded)\n",
            "- 5 tablespoons Ice water\n",
            "- 80 milliliters milk\n",
            "- Dash of white pepper\n",
            "- 200 grams Plain flour\n",
            "- 1/8 teaspoon salt\n",
            "- 1/2 tablespoon Shallots (sliced)\n",
            "- 125 grams Spinach (wash & drain well, shredded)\n",
            "- 80 milliliters Whipping cream\n",
            "\n",
            "### Instructions:\n",
            "1. With a little oil, fry the shallot till fragrant and golden brown.\n",
            "2. Add the ham and fry for a while, then add the spinach and fry until the spinach is cooked. Dish up and set aside.\n",
            "3. Measure the whipping cream and milk together in a measuring cup (you can omit whipping cream and use all 160ml milk). Add eggs (lightly beaten) and a pinch of salt, then mix well.\n",
            "4. Spread the ham, spinach, and cheddar cheese evenly over the bottom of the prepared tart pan. Add a dash of pepper, then gently pour the egg mixture over it.\n",
            "5. Bake in a preheated oven at 180°C for 40 minutes and serve hot or warm.\n",
            "\n",
            "You can find more details [here](https://www.foodista.com/recipe/Z7KLXBR8/spinach-ham-quiche).\n",
            "\n",
            "Enjoy your picnic, Arsenii! If you need anything else, just let me know!\n",
            "[Gradio Backend] Returning final thread ID: thread_GTDiTiMrgYV9370CqFkyjyJB\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://a1f9cb72f470ea7a0b.gradio.live\n",
            "\n",
            "Gradio Interface launched. Interact with the chat window above.\n"
          ]
        }
      ]
    }
  ]
}